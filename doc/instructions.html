<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CNN practical</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body><div class="container"><h1 id="vgg-convolutional-neural-netwroks-practrical">VGG Convolutional Neural Netwroks Practrical</h1>

<p><em>By Andrea Vedaldi and Andrew Zisserman</em></p>

<p>This is an <a href="http://www.robots.ox.ac.uk/~vgg">Oxford Visual Geometry Group</a> computer vision practical, authored by <a href="http://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a> and Andrew Zisserman (Release 2015a).</p>

<p><img height="400px" src="images/cover.png" alt="cover"></p>

<p><em>Convolutional neural networks</em> are an important class of learnable representations applicable, among others, to numerous computer vision problems. Deep CNNs, in particular, hare composed of several layers of processing, each involving linear as well as non-linear operators, that are learned jointly, in an end-to-end manner, to solve a particular tasks. Such methods become the predominant for feature extraction from audiovisual and textual data.</p>

<p>This practical explores the basics of learning (deep) CNNs. The first part introduces typical CNN building blocks, such as ReLU units and linear filters, with a particular emphasis on understanding back-propagation. The second part looks at learning two basic CNNs. The first one is a simple non-linear filetr capturing particular image structures, while the second one is a network that recognises typewritten characters (using a variety of different fonts). These examples illustrate the use of stochastic gradient descent with momentum, the definition of an objective function, the construction of mini-batches of data, and data jittering. The last part shows how powerful CNN models can be downloaded off-the-shelf and used directly in applications, bypassing the expensive trianing process.</p>

<p><div class="toc"><div class="toc">
<ul>
<li><a href="#vgg-convolutional-neural-netwroks-practrical">VGG Convolutional Neural Netwroks Practrical</a><ul>
<li><a href="#getting-started">Getting started</a></li>
<li><a href="#part1">Part 1: CNN building blocks</a><ul>
<li><a href="#part1-1">Part 1.1: linear convolution</a></li>
<li><a href="#part1-2">Part 1.2: non-linear gating</a></li>
<li><a href="#part1-3">Part 1.3: pooling</a></li>
<li><a href="#part1-4">Part 1.4: normalisation</a></li>
</ul>
</li>
<li><a href="#part-2-back-propagation-and-derivatives">Part 2: back-propagation and derivatives</a><ul>
<li><a href="#part-21-theory-of-backpropagation">Part 2.1: theory of backpropagation</a></li>
<li><a href="#part-21-putting-backpropagation-to-the-practice">Part 2.1: putting backpropagation to the practice</a></li>
</ul>
</li>
<li><a href="#part-3-learning-a-tiny-cnn">Part 3: learning a tiny CNN</a><ul>
<li><a href="#part-31-training-data-and-labels">Part 3.1: training data and labels</a></li>
<li><a href="#part-32-image-preprocessing">Part 3.2: image preprocessing</a></li>
<li><a href="#part-33-learning-with-gradient-descent">Part 3.3: learning with gradient descent</a></li>
<li><a href="#part-34-experimenting-with-the-tiny-cnn">Part 3.4: experimenting with the tiny CNN</a></li>
</ul>
</li>
<li><a href="#part-4-learning-a-character-cnn">Part 4: learning a character CNN</a><ul>
<li><a href="#part-41-prepare-the-data">Part 4.1: prepare the data</a></li>
<li><a href="#part-42-intialize-a-cnn-architecture">Part 4.2: intialize a CNN architecture</a></li>
<li><a href="#part-43-train-and-evaluate-the-cnn">Part 4.3: train and evaluate the CNN</a></li>
<li><a href="#part-44-visualize-the-learned-filters">Part 4.4: visualize the learned filters</a></li>
<li><a href="#part-45-apply-the-model">Part 4.5: apply the model</a></li>
<li><a href="#part-46-training-with-jitter">Part 4.6: training with jitter</a></li>
</ul>
</li>
<li><a href="#part-5-using-pertained-models">Part 5: using pertained models</a><ul>
<li><a href="#part-51-load-a-pretrained-model">Part 5.1:  load a pretrained model</a></li>
<li><a href="#part-52-use-the-model-to-classify-an-image">Part 5.2: use the model to classify an image</a></li>
</ul>
</li>
<li><a href="#links-and-further-work">Links and further work</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#history">History</a></li>
</ul>
</li>
</ul>
</div>
</div>
</p>



<p><span class="MathJax_Preview"></span><div class="MathJax_Display" role="textbox" aria-readonly="true" style="text-align: center;"><span class="MathJax" id="MathJax-Element-10-Frame" style=""><nobr><span class="math" id="MathJax-Span-108" style="width: 0.014em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(3.869em, 1000em, 4.167em, -0.446em); top: -4.018em; left: 0em;"><span class="mrow" id="MathJax-Span-109"></span><span style="display: inline-block; width: 0px; height: 4.018em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.111em; vertical-align: -0.056em;"></span></span></nobr></span></div><script type="math/tex; mode=display" id="MathJax-Element-10">
   \newcommand{\bx}{\mathbf{x}}
   \newcommand{\by}{\mathbf{y}}
   \newcommand{\bz}{\mathbf{z}}
   \newcommand{\bw}{\mathbf{w}}
   \newcommand{\cP}{\mathcal{P}}
   \newcommand{\cN}{\mathcal{N}}
   \newcommand{\vc}{\operatorname{vec}}
</script></p>



<h2 id="getting-started">Getting started</h2>

<p>Read and understand the <a href="../overview/index.html#installation">requirements and installation instructions</a>. The download links for this practical are:</p>

<ul>
<li>Code and data: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-cnn-2015a.tar.gz">practical-cnn-2015a.tar.gz</a></li>
<li>Code only: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-cnn-2015a-code-only.tar.gz">practical-cnn-2015a-code-only.tar.gz</a></li>
<li>Data only: <a href="http://www.robots.ox.ac.uk/~vgg/share/practical-cnn-2015a-data-only.tar.gz">practical-cnn-2015a-data-only.tar.gz</a></li>
<li><a href="https://github.com/vedaldi/practical-cnn">Git repository</a> (for lab setters and developers)</li>
</ul>

<p>After the installation is complete, open and edit the script <code>exercise1.m</code> in the MATLAB editor. The script contains commented code and a description for all steps of this exercise, relative to <a href="#part1">Part I</a> of this document. You can cut and paste this code into the MATLAB window to run it, and will need to modify it as you go through the session. Other files <code>exercise2.m</code>, <code>exercise3.m</code>, and <code>exercise4.m</code> are given for <a href="#part2">Part II</a>, <a href="#part3">III</a>, and <a href="part4">IV</a>.</p>

<p>Each part contains several <strong>Questions</strong> (that require pen and paper) and <strong>Tasks</strong> (that require experimentation or coding) to be answered/completed before proceeding further in the practical.</p>

<h2 id="part1">Part 1: CNN building blocks</h2>



<h3 id="part1-1" class="1">Part 1.1: linear convolution</h3>

<p>A feed-forward neural network can be thought of as the composition of number of functions  <br>
<span class="MathJax_Preview"></span><div class="MathJax_Display" role="textbox" aria-readonly="true" style="text-align: center;"><span class="MathJax" id="MathJax-Element-11-Frame" style=""><nobr><span class="math" id="MathJax-Span-110" style="width: 17.292em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.427em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.653em, 1000em, 2.871em, -0.593em); top: -2.48em; left: 0em;"><span class="mrow" id="MathJax-Span-111"><span class="mi" id="MathJax-Span-112" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span class="mo" id="MathJax-Span-113" style="font-family: STIXGeneral-Regular;">(</span><span class="texatom" id="MathJax-Span-114"><span class="mrow" id="MathJax-Span-115"><span class="mi" id="MathJax-Span-116" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span class="mo" id="MathJax-Span-117" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-118" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">=</span><span class="msubsup" id="MathJax-Span-119" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 0.772em; height: 0px;"><span style="position: absolute; clip: rect(1.653em, 1000em, 2.836em, -0.593em); top: -2.48em; left: 0em;"><span class="mi" id="MathJax-Span-120" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.039em; left: 0.3em;"><span class="mi" id="MathJax-Span-121" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-122" style="font-family: STIXGeneral-Regular;">(</span><span class="mo" id="MathJax-Span-123" style="font-family: STIXGeneral-Regular;">…</span><span class="msubsup" id="MathJax-Span-124" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 0.723em; height: 0px;"><span style="position: absolute; clip: rect(1.653em, 1000em, 2.836em, -0.593em); top: -2.48em; left: 0em;"><span class="mi" id="MathJax-Span-125" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.039em; left: 0.3em;"><span class="mn" id="MathJax-Span-126" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-127" style="font-family: STIXGeneral-Regular;">(</span><span class="msubsup" id="MathJax-Span-128"><span style="display: inline-block; position: relative; width: 0.723em; height: 0px;"><span style="position: absolute; clip: rect(1.653em, 1000em, 2.836em, -0.593em); top: -2.48em; left: 0em;"><span class="mi" id="MathJax-Span-129" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.039em; left: 0.3em;"><span class="mn" id="MathJax-Span-130" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-131" style="font-family: STIXGeneral-Regular;">(</span><span class="texatom" id="MathJax-Span-132"><span class="mrow" id="MathJax-Span-133"><span class="mi" id="MathJax-Span-134" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span class="mo" id="MathJax-Span-135" style="font-family: STIXGeneral-Regular;">;</span><span class="msubsup" id="MathJax-Span-136" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.166em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.643em, -0.423em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-137"><span class="mrow" id="MathJax-Span-138"><span class="mi" id="MathJax-Span-139" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.744em;"><span class="mn" id="MathJax-Span-140" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-141" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-142" style="font-family: STIXGeneral-Regular;">;</span><span class="msubsup" id="MathJax-Span-143" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.166em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.643em, -0.423em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-144"><span class="mrow" id="MathJax-Span-145"><span class="mi" id="MathJax-Span-146" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.744em;"><span class="mn" id="MathJax-Span-147" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">2</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-148" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-149" style="font-family: STIXGeneral-Regular; padding-left: 0.188em;">…</span><span class="mo" id="MathJax-Span-150" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-151" style="font-family: STIXGeneral-Regular;">,</span><span class="msubsup" id="MathJax-Span-152" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.216em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.643em, -0.423em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-153"><span class="mrow" id="MathJax-Span-154"><span class="mi" id="MathJax-Span-155" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.744em;"><span class="texatom" id="MathJax-Span-156"><span class="mrow" id="MathJax-Span-157"><span class="mi" id="MathJax-Span-158" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-159" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-160" style="font-family: STIXGeneral-Regular;">.</span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.142em; vertical-align: -0.327em;"></span></span></nobr></span></div><script type="math/tex; mode=display" id="MathJax-Element-11">
f(\bx) = f_L(\dots f_2(f_1(\bx;\bw_1);\bw_2)\dots),\bw_{L}).
</script> <br>
Each function <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-12-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-161" style="width: 0.681em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.595em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.405em, 1000em, 2.631em, -0.593em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-162"><span class="msubsup" id="MathJax-Span-163"><span style="display: inline-block; position: relative; width: 0.574em; height: 0px;"><span style="position: absolute; clip: rect(1.653em, 1000em, 2.836em, -0.593em); top: -2.48em; left: 0em;"><span class="mi" id="MathJax-Span-164" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.039em; left: 0.3em;"><span class="mi" id="MathJax-Span-165" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.001em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.151em; vertical-align: -0.336em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-12">f_l</script> takes as input a datum <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-13-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-166" style="width: 0.903em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.794em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.622em, 1000em, 2.539em, -0.434em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-167"><span class="msubsup" id="MathJax-Span-168"><span style="display: inline-block; position: relative; width: 0.769em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.629em, -0.434em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-169"><span class="mrow" id="MathJax-Span-170"><span class="mi" id="MathJax-Span-171" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.496em;"><span class="mi" id="MathJax-Span-172" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.001em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.804em; vertical-align: -0.232em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-13">\bx_l</script> and a parameter vector <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-14-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-173" style="width: 1.181em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.042em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.622em, 1000em, 2.539em, -0.423em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-174"><span class="msubsup" id="MathJax-Span-175"><span style="display: inline-block; position: relative; width: 1.017em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.643em, -0.423em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-176"><span class="mrow" id="MathJax-Span-177"><span class="mi" id="MathJax-Span-178" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.744em;"><span class="mi" id="MathJax-Span-179" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.001em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.804em; vertical-align: -0.232em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-14">\bw_l</script> and produces as output a datum <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-15-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-180" style="width: 1.847em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.637em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.622em, 1000em, 2.56em, -0.434em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-181"><span class="msubsup" id="MathJax-Span-182"><span style="display: inline-block; position: relative; width: 1.613em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.629em, -0.434em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-183"><span class="mrow" id="MathJax-Span-184"><span class="mi" id="MathJax-Span-185" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.496em;"><span class="texatom" id="MathJax-Span-186"><span class="mrow" id="MathJax-Span-187"><span class="mi" id="MathJax-Span-188" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.001em;"></span></span><span class="mo" id="MathJax-Span-189" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">+</span><span class="mn" id="MathJax-Span-190" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.828em; vertical-align: -0.256em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-15">\bx_{l+1}</script>. While the type and sequence of functions is usually handcrafted, the paramters <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-16-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-191" style="width: 8.014em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.143em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.655em, 1000em, 2.806em, -0.423em); top: -2.48em; left: 0em;"><span class="mrow" id="MathJax-Span-192"><span class="texatom" id="MathJax-Span-193"><span class="mrow" id="MathJax-Span-194"><span class="mi" id="MathJax-Span-195" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span class="mo" id="MathJax-Span-196" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">=</span><span class="mo" id="MathJax-Span-197" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">(</span><span class="msubsup" id="MathJax-Span-198"><span style="display: inline-block; position: relative; width: 1.166em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.643em, -0.423em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-199"><span class="mrow" id="MathJax-Span-200"><span class="mi" id="MathJax-Span-201" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.744em;"><span class="mn" id="MathJax-Span-202" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-203" style="font-family: STIXGeneral-Regular;">,</span><span class="mo" id="MathJax-Span-204" style="font-family: STIXGeneral-Regular; padding-left: 0.188em;">…</span><span class="mo" id="MathJax-Span-205" style="font-family: STIXGeneral-Regular; padding-left: 0.188em;">,</span><span class="msubsup" id="MathJax-Span-206" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.216em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.643em, -0.423em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-207"><span class="mrow" id="MathJax-Span-208"><span class="mi" id="MathJax-Span-209" style="font-family: STIXGeneral; font-weight: bold;">w</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.744em;"><span class="mi" id="MathJax-Span-210" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">L<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-211" style="font-family: STIXGeneral-Regular;">)</span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.066em; vertical-align: -0.254em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-16">\bw=(\bw_1,\dots,\bw_L)</script> are <em>learned from data</em> in order to solve a target problem, for example classifying images or sounds.</p>

<p>In a <em>convolutional neural network</em> data and functions have additional structure. The data <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-17-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-212" style="width: 4.403em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.919em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.785em, -0.434em); top: -2.48em; left: 0em;"><span class="mrow" id="MathJax-Span-213"><span class="msubsup" id="MathJax-Span-214"><span style="display: inline-block; position: relative; width: 0.918em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.629em, -0.434em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-215"><span class="mrow" id="MathJax-Span-216"><span class="mi" id="MathJax-Span-217" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.496em;"><span class="mn" id="MathJax-Span-218" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-219" style="font-family: STIXGeneral-Regular;">,</span><span class="mo" id="MathJax-Span-220" style="font-family: STIXGeneral-Regular; padding-left: 0.188em;">…</span><span class="mo" id="MathJax-Span-221" style="font-family: STIXGeneral-Regular; padding-left: 0.188em;">,</span><span class="msubsup" id="MathJax-Span-222" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 0.918em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.629em, -0.434em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-223"><span class="mrow" id="MathJax-Span-224"><span class="mi" id="MathJax-Span-225" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.496em;"><span class="mi" id="MathJax-Span-226" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">n</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.803em; vertical-align: -0.231em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-17">\bx_1,\dots,\bx_n</script> are images, sounds, or more in general maps from a lattice<a href="#fn:lattice" id="fnref:lattice" title="See footnote" class="footnote">1</a> to one or more real numbers. In particular, since the rest of the practical will focus on computer vision applications, data will be 2D dimensional arrays of pixels. Formally, each <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-18-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-227" style="width: 0.903em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.794em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.622em, 1000em, 2.539em, -0.434em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-228"><span class="msubsup" id="MathJax-Span-229"><span style="display: inline-block; position: relative; width: 0.769em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.629em, -0.434em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-230"><span class="mrow" id="MathJax-Span-231"><span class="mi" id="MathJax-Span-232" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.496em;"><span class="mi" id="MathJax-Span-233" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">i</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.804em; vertical-align: -0.232em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-18">\bx_i</script> will be a <span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-19-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-234" style="width: 5.181em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.613em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.678em, 1000em, 2.654em, -0.464em); top: -2.48em; left: 0em;"><span class="mrow" id="MathJax-Span-235"><span class="mi" id="MathJax-Span-236" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.039em;"></span></span><span class="mo" id="MathJax-Span-237" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">×</span><span class="mi" id="MathJax-Span-238" style="font-family: STIXGeneral-Italic; padding-left: 0.25em;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.06em;"></span></span><span class="mo" id="MathJax-Span-239" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">×</span><span class="mi" id="MathJax-Span-240" style="font-family: STIXGeneral-Italic; padding-left: 0.25em;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.87em; vertical-align: -0.084em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-19">M \times N \times K</script> real array of <span class="MathJax_Preview">M \times N</span><span class="MathJax MathJax_Processed" id="MathJax-Element-20-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-241" style="width: 3.069em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.728em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.678em, 1000em, 2.654em, -0.464em); top: -2.48em; left: 0em;"><span class="mrow" id="MathJax-Span-242"><span class="mi" id="MathJax-Span-243" style="font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.039em;"></span></span><span class="mo" id="MathJax-Span-244" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">×</span><span class="mi" id="MathJax-Span-245" style="font-family: STIXGeneral-Italic; padding-left: 0.25em;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.06em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.87em; vertical-align: -0.084em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-20">M \times N</script> pixels and <span class="MathJax_Preview">K</span><span class="MathJax MathJax_Processed" id="MathJax-Element-21-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-246" style="width: 0.847em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.744em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.678em, 1000em, 2.629em, -0.439em); top: -2.48em; left: 0em;"><span class="mrow" id="MathJax-Span-247"><span class="mi" id="MathJax-Span-248" style="font-family: STIXGeneral-Italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.055em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.842em; vertical-align: -0.056em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-21">K</script> channels per pixels. Hence the first two dimension of the array span space, while the last one channels. Note that only the input <span class="MathJax_Preview">\bx=\bx_1</span><span class="MathJax MathJax_Processed" id="MathJax-Element-22-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-249" style="width: 3.069em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.728em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.779em, -0.434em); top: -2.48em; left: 0em;"><span class="mrow" id="MathJax-Span-250"><span class="texatom" id="MathJax-Span-251"><span class="mrow" id="MathJax-Span-252"><span class="mi" id="MathJax-Span-253" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span class="mo" id="MathJax-Span-254" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">=</span><span class="msubsup" id="MathJax-Span-255" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 0.918em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.629em, -0.434em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-256"><span class="mrow" id="MathJax-Span-257"><span class="mi" id="MathJax-Span-258" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.496em;"><span class="mn" id="MathJax-Span-259" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">1</span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.795em; vertical-align: -0.224em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-22">\bx=\bx_1</script> of the network is an actual image, while the remaining data are intermediate <em>feature maps</em>.</p>

<p>The second property of a CNN is that the functions <span class="MathJax_Preview">f_l</span><span class="MathJax MathJax_Processed" id="MathJax-Element-23-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-260" style="width: 0.681em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.595em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.405em, 1000em, 2.631em, -0.593em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-261"><span class="msubsup" id="MathJax-Span-262"><span style="display: inline-block; position: relative; width: 0.574em; height: 0px;"><span style="position: absolute; clip: rect(1.653em, 1000em, 2.836em, -0.593em); top: -2.48em; left: 0em;"><span class="mi" id="MathJax-Span-263" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.039em; left: 0.3em;"><span class="mi" id="MathJax-Span-264" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.001em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.151em; vertical-align: -0.336em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-23">f_l</script> have a <em>convolutional structure</em>. This means that <span class="MathJax_Preview">f_l</span><span class="MathJax MathJax_Processed" id="MathJax-Element-24-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-265" style="width: 0.681em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.595em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.405em, 1000em, 2.631em, -0.593em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-266"><span class="msubsup" id="MathJax-Span-267"><span style="display: inline-block; position: relative; width: 0.574em; height: 0px;"><span style="position: absolute; clip: rect(1.653em, 1000em, 2.836em, -0.593em); top: -2.48em; left: 0em;"><span class="mi" id="MathJax-Span-268" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.039em; left: 0.3em;"><span class="mi" id="MathJax-Span-269" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.001em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.151em; vertical-align: -0.336em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-24">f_l</script> applies to the input map <span class="MathJax_Preview">\bx_l</span><span class="MathJax MathJax_Processed" id="MathJax-Element-25-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-270" style="width: 0.903em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.794em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.622em, 1000em, 2.539em, -0.434em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-271"><span class="msubsup" id="MathJax-Span-272"><span style="display: inline-block; position: relative; width: 0.769em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.629em, -0.434em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-273"><span class="mrow" id="MathJax-Span-274"><span class="mi" id="MathJax-Span-275" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.496em;"><span class="mi" id="MathJax-Span-276" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.001em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.804em; vertical-align: -0.232em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-25">\bx_l</script> an operator that is <em>local and translation invariant</em>. Examples of convolutional operators are applying a bank of linear filters to <span class="MathJax_Preview">\bx_l</span><span class="MathJax MathJax_Processed" id="MathJax-Element-26-Frame" role="textbox" aria-readonly="true" style=""><nobr><span class="math" id="MathJax-Span-277" style="width: 0.903em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.794em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.622em, 1000em, 2.539em, -0.434em); top: -2.232em; left: 0em;"><span class="mrow" id="MathJax-Span-278"><span class="msubsup" id="MathJax-Span-279"><span style="display: inline-block; position: relative; width: 0.769em; height: 0px;"><span style="position: absolute; clip: rect(1.87em, 1000em, 2.629em, -0.434em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-280"><span class="mrow" id="MathJax-Span-281"><span class="mi" id="MathJax-Span-282" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.132em; left: 0.496em;"><span class="mi" id="MathJax-Span-283" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">l<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.001em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 0.804em; vertical-align: -0.232em;"></span></span></nobr></span><script type="math/tex" id="MathJax-Element-26">\bx_l</script> or computing its element-wise square. </p>

<p>In this part we will familiarise ourselves with a number of such convolutional operators. The first one is the regular <em>linear convolution</em> by a filter bank. We will start by focusing our attention on a single function relation as follows: <br>
<span class="MathJax_Preview">
 f: \mathbb{R}^{M\times N\times K} \rightarrow \mathbb{R}^{M' \times N' \times K'},
 \qquad \bx \mapsto \by.
</span><div class="MathJax_Display MathJax_Processed" role="textbox" aria-readonly="true" style="text-align: center;"><span class="MathJax" id="MathJax-Element-27-Frame" style=""><nobr><span class="math" id="MathJax-Span-284" style="width: 17.903em; display: inline-block;"><span style="display: inline-block; position: relative; width: 15.972em; height: 0px; font-size: 112%;"><span style="position: absolute; clip: rect(1.367em, 1000em, 2.836em, -0.593em); top: -2.48em; left: 0em;"><span class="mrow" id="MathJax-Span-285"><span class="mi" id="MathJax-Span-286" style="font-family: STIXGeneral-Italic;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span class="mo" id="MathJax-Span-287" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">:</span><span class="msubsup" id="MathJax-Span-288" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 3.299em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000em, 2.629em, -0.376em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-289"><span class="mrow" id="MathJax-Span-290"><span class="mi" id="MathJax-Span-291" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.695em; left: 0.744em;"><span class="texatom" id="MathJax-Span-292"><span class="mrow" id="MathJax-Span-293"><span class="mi" id="MathJax-Span-294" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.028em;"></span></span><span class="mo" id="MathJax-Span-295" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-296" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.042em;"></span></span><span class="mo" id="MathJax-Span-297" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="mi" id="MathJax-Span-298" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.039em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-299" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">→</span><span class="msubsup" id="MathJax-Span-300" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 4.291em; height: 0px;"><span style="position: absolute; clip: rect(1.669em, 1000em, 2.629em, -0.376em); top: -2.48em; left: 0em;"><span class="texatom" id="MathJax-Span-301"><span class="mrow" id="MathJax-Span-302"><span class="mi" id="MathJax-Span-303" style="font-family: STIXGeneral-Regular;">ℝ</span></span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span><span style="position: absolute; top: -2.695em; left: 0.744em;"><span class="texatom" id="MathJax-Span-304"><span class="mrow" id="MathJax-Span-305"><span class="msup" id="MathJax-Span-306"><span style="display: inline-block; position: relative; width: 0.955em; height: 0px;"><span style="position: absolute; clip: rect(1.671em, 1000em, 2.431em, -0.459em); top: -2.282em; left: 0em;"><span class="mi" id="MathJax-Span-307" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">M<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.028em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span><span style="position: absolute; top: -2.501em; left: 0.703em;"><span class="mo" id="MathJax-Span-308" style="font-size: 50%; font-family: STIXVariants;">′</span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span></span><span class="mo" id="MathJax-Span-309" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="msup" id="MathJax-Span-310"><span style="display: inline-block; position: relative; width: 0.81em; height: 0px;"><span style="position: absolute; clip: rect(1.671em, 1000em, 2.441em, -0.461em); top: -2.282em; left: 0em;"><span class="mi" id="MathJax-Span-311" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">N<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.042em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span><span style="position: absolute; top: -2.501em; left: 0.559em;"><span class="mo" id="MathJax-Span-312" style="font-size: 50%; font-family: STIXVariants;">′</span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span></span><span class="mo" id="MathJax-Span-313" style="font-size: 70.7%; font-family: STIXGeneral-Regular;">×</span><span class="msup" id="MathJax-Span-314"><span style="display: inline-block; position: relative; width: 0.809em; height: 0px;"><span style="position: absolute; clip: rect(1.671em, 1000em, 2.431em, -0.441em); top: -2.282em; left: 0em;"><span class="mi" id="MathJax-Span-315" style="font-size: 70.7%; font-family: STIXGeneral-Italic;">K<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.039em;"></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span><span style="position: absolute; top: -2.501em; left: 0.558em;"><span class="mo" id="MathJax-Span-316" style="font-size: 50%; font-family: STIXVariants;">′</span><span style="display: inline-block; width: 0px; height: 2.232em;"></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.282em;"></span></span></span></span><span class="mo" id="MathJax-Span-317" style="font-family: STIXGeneral-Regular;">,</span><span class="mspace" id="MathJax-Span-318" style="height: 0em; vertical-align: 0em; width: 2.25em; display: inline-block; overflow: hidden;"></span><span class="texatom" id="MathJax-Span-319" style="padding-left: 0.188em;"><span class="mrow" id="MathJax-Span-320"><span class="mi" id="MathJax-Span-321" style="font-family: STIXGeneral; font-weight: bold;">x</span></span></span><span class="mo" id="MathJax-Span-322" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">↦</span><span class="texatom" id="MathJax-Span-323" style="padding-left: 0.313em;"><span class="mrow" id="MathJax-Span-324"><span class="mi" id="MathJax-Span-325" style="font-family: STIXGeneral; font-weight: bold;">y</span></span></span><span class="mo" id="MathJax-Span-326" style="font-family: STIXGeneral-Regular;">.</span></span><span style="display: inline-block; width: 0px; height: 2.48em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.423em; vertical-align: -0.287em;"></span></span></nobr></span></div><script type="math/tex; mode=display" id="MathJax-Element-27">
 f: \mathbb{R}^{M\times N\times K} \rightarrow \mathbb{R}^{M' \times N' \times K'},
 \qquad \bx \mapsto \by.
</script> <br>
Open the <code>example1.m</code> file, select the following part of the code, and execute it in MATLAB (right button &gt; <code>Evaluate selection</code> or <code>Shift+F7</code>).</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Read an example image</span>
x = imread(<span class="hljs-string">'peppers.png'</span>) ;

<span class="hljs-comment">% Convert to single format</span>
x = im2single(x) ;

<span class="hljs-comment">% Visualize the input x</span>
figure(<span class="hljs-number">1</span>) ; clf ; imagesc(x) </code></pre>

<p>This should display an image of bell peppers in Figure 1:</p>

<p>![bell][2]</p>

<p>Use MATLAB <code>size</code> command to obtain the size of the array <code>x</code>. Note that the array <code>x</code> is converted to <em>single precision</em> format. This is because the underlying MatConvNet assumes that data is in single precision.</p>

<blockquote>
  <p><strong>Question.</strong> The third dimension of <code>x</code> is 3. Why?</p>
</blockquote>

<p>Now we will create a bank 10 of <span class="MathJax_Preview">5 \times 5 \times 3</span><span class="MathJax MathJax_Processing" id="MathJax-Element-28-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-28">5 \times 5 \times 3</script> filters.</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Create a bank of linear filters</span>
w = <span class="hljs-built_in">randn</span>(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,<span class="hljs-number">10</span>,<span class="hljs-string">'single'</span>) ;</code></pre>

<p>The filters are in single precision as well. Note that <code>w</code> has four dimensions, packing 10 filters. Note also that each filter is not flat, but rather a volume with three layers. The next step is applying the filter to the image. This uses the <code>vl_nnconv</code> function from MatConvNet:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Apply the convolutional operator</span>
y = vl_nnconv(x, w, <span class="hljs-matrix">[]</span>) ;</code></pre>

<p><strong>Remark:</strong> You might have noticed that the third argument to the <code>vl_nnconv</code> function is the empty matrix <code>[]</code>. It can be otherwise used to pass a vector of bias terms to add to the output of each filter.</p>

<p>The variable <code>y</code> contains the output of the convolution. Note that the filters are three-dimensional, in the sense that it operates on a map <span class="MathJax_Preview">\bx</span><span class="MathJax MathJax_Processing" id="MathJax-Element-29-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-29">\bx</script> with <span class="MathJax_Preview">K</span><span class="MathJax MathJax_Processing" id="MathJax-Element-30-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-30">K</script> channels. Furthermore, there are <span class="MathJax_Preview">K'</span><span class="MathJax MathJax_Processing" id="MathJax-Element-31-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-31">K'</script> such filters, generating a <span class="MathJax_Preview">K'</span><span class="MathJax MathJax_Processing" id="MathJax-Element-32-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-32">K'</script> dimensional map <span class="MathJax_Preview">\by</span><span class="MathJax MathJax_Processing" id="MathJax-Element-33-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-33">\by</script> as follows <br>
<span class="MathJax_Preview">
y_{i'j'k'} = \sum_{ijk} w_{ijk} x_{i+i',j+j',k,k'}
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-34-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-34">
y_{i'j'k'} = \sum_{ijk} w_{ijk} x_{i+i',j+j',k,k'}
</script></p>

<blockquote>
  <p><strong>Questions:</strong> Study carefully this expression and answer the following:</p>
  
  <ul>
  <li>Given that the input map <span class="MathJax_Preview">\bx</span><span class="MathJax MathJax_Processing" id="MathJax-Element-35-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-35">\bx</script> has <span class="MathJax_Preview">M \times N \times K</span><span class="MathJax MathJax_Processing" id="MathJax-Element-36-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-36">M \times N \times K</script> dimensions and that each of the <span class="MathJax_Preview">K'</span><span class="MathJax MathJax_Processing" id="MathJax-Element-37-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-37">K'</script> filters has dimension <span class="MathJax_Preview">M_f \times N_f \times K</span><span class="MathJax MathJax_Processing" id="MathJax-Element-38-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-38">M_f \times N_f \times K</script>, what is the dimension of <span class="MathJax_Preview">\by</span><span class="MathJax MathJax_Processing" id="MathJax-Element-39-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-39">\by</script>?</li>
  <li>Note that <span class="MathJax_Preview">x</span><span class="MathJax MathJax_Processing" id="MathJax-Element-40-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-40">x</script> is indexed by <span class="MathJax_Preview">i+i'</span><span class="MathJax MathJax_Processing" id="MathJax-Element-41-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-41">i+i'</script> and <span class="MathJax_Preview">j+j'</span><span class="MathJax MathJax_Processing" id="MathJax-Element-42-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-42">j+j'</script>, but that there is no plus sign between <span class="MathJax_Preview">k</span><span class="MathJax MathJax_Processing" id="MathJax-Element-43-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-43">k</script> and <span class="MathJax_Preview">k'</span><span class="MathJax MathJax_Processing" id="MathJax-Element-44-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-44">k'</script>. Why?</li>
  </ul>
  
  <p><strong>Task:</strong> check that the size of the variable <code>y</code> matches your calculations.</p>
</blockquote>

<p>We can now visualize the output <code>y</code> of the convolution. In order to do this, use the <a href="http://www.vlfeat.org/matlab/vl_imarraysc.html"><code>vl_imarraysc</code></a> function to display an image for each feature channel in <code>y</code>:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Visualize the output y</span>
figure(<span class="hljs-number">2</span>) ; clf ; vl_imarraysc(y) ; colormap gray ;</code></pre>

<blockquote>
  <p><strong>Question:</strong> Study the feature channels obtained. Most will likely contain a strong response in correspondences of edges in the input image <code>x</code>. Recall that <code>w</code> was obtained by drawing at random numbers from a Gaussian distribution. Can explain this phenomenon?</p>
</blockquote>

<p>So far filters preserve the resolution of the input feature map. However, it is often useful to <em>dowsnample the output</em>. This can be obtained by using the <code>stride</code> option in <code>vl_nnconv</code>:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Try again, downsampling the output</span>
y_ds = vl_nnconv(x, w, <span class="hljs-matrix">[]</span>, <span class="hljs-string">'stride'</span>, <span class="hljs-number">16</span>) ;
figure(<span class="hljs-number">3</span>) ; clf ; vl_imarraysc(y_ds) ; colormap gray ;</code></pre>

<p>As you should have noticed in a question above, applying a filter to an image or feature map interacts with the boundaries, making the output map smaller by an amount proportional to the size of the filters. If this is undesirable, then the input array can be padded with zeros by using the <code>pad</code> option:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Try padding</span>
y_pad = vl_nnconv(x, w, <span class="hljs-matrix">[]</span>, <span class="hljs-string">'pad'</span>, <span class="hljs-number">4</span>) ;
figure(<span class="hljs-number">4</span>) ; clf ; vl_imarraysc(y_pad) ; colormap gray ;</code></pre>

<blockquote>
  <p><strong>Task:</strong> Convince yourself that the previous code’s output has different boundaries compared to the code that does not use padding. Can you justify the result?</p>
</blockquote>

<p>In order to consolidate what learned so far, we will now design a filter by hand:</p>



<pre class="prettyprint"><code class="language-matlab hljs ">w = <span class="hljs-matrix">[<span class="hljs-number">0</span>  <span class="hljs-number">1</span> <span class="hljs-number">0</span> ;
     <span class="hljs-number">1</span> -<span class="hljs-number">4</span> <span class="hljs-number">1</span> ;
     <span class="hljs-number">0</span>  <span class="hljs-number">1</span> <span class="hljs-number">0</span> ]</span> ;
w = single(<span class="hljs-built_in">repmat</span>(w, <span class="hljs-matrix">[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>]</span>)) ;
y_lap = vl_nnconv(x, w, <span class="hljs-matrix">[]</span>) ;

figure(<span class="hljs-number">5</span>) ; clf ; colormap gray ;
subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>) ; 
imagesc(y_lap) ; title(<span class="hljs-string">'filter output'</span>) ;
subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>) ;
imagesc(-<span class="hljs-built_in">abs</span>(y_lap)) ; title(<span class="hljs-string">'- abs(filter output)'</span>) ;</code></pre>

<blockquote>
  <p><strong>Questions:</strong></p>
  
  <ul>
  <li>What filter have we implemented?</li>
  <li>How are the RGB colour channels processed by this filter?</li>
  <li>What image structure are detected?</li>
  </ul>
</blockquote>



<h3 id="part1-2" class="2">Part 1.2: non-linear gating</h3>

<p>As we have stated in the introduction, CNNs are obtained by composing several different functions. In addition to the linear filters shown in the <a href="#part1.1">previous part</a>, there are several non-linear convolutonal operators as well.</p>

<blockquote>
  <p><strong>Question:</strong> Some of the functions in a CNN <em>must</em> be non-linear. Why?</p>
</blockquote>

<p>The simplest non-linearity is obtained by following a linear filter by a <em>non-linear gating function</em>, applied identically to each component of a feature map. The simplest such function is the <em>Rectified Linear Unit (ReLU)</em> <br>
<span class="MathJax_Preview">
  y_{ijk} = \max\{0, x_{ijk}\}.
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-45-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-45">
  y_{ijk} = \max\{0, x_{ijk}\}.
</script> <br>
This function is implemented by <code>vl_relu</code>; let’s try this out:</p>



<pre class="prettyprint"><code class="language-matlab hljs ">w = single(<span class="hljs-built_in">repmat</span>(<span class="hljs-matrix">[<span class="hljs-number">1</span> <span class="hljs-number">0</span> -<span class="hljs-number">1</span>]</span>, <span class="hljs-matrix">[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>]</span>)) ;
w = <span class="hljs-built_in">cat</span>(<span class="hljs-number">4</span>, w, -w) ;
y = vl_nnconv(x, w, <span class="hljs-matrix">[]</span>) ;
z = vl_nnrelu(y) ;

figure(<span class="hljs-number">6</span>) ; clf ; colormap gray ;
subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>) ; vl_imarraysc(y) ;
subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>) ; vl_imarraysc(z) ;</code></pre>

<blockquote>
  <p><strong>Tasks:</strong></p>
  
  <ul>
  <li>Run the code above and understand what the filter <span class="MathJax_Preview">\bw</span><span class="MathJax MathJax_Processing" id="MathJax-Element-46-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-46">\bw</script> is doing.</li>
  <li>Explain the final result <span class="MathJax_Preview">\bz</span><span class="MathJax MathJax_Processing" id="MathJax-Element-47-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-47">\bz</script>.</li>
  </ul>
</blockquote>



<h3 id="part1-3" class="3">Part 1.3: pooling</h3>

<p>There are several other important operators in a CNN. One of them is <em>pooling</em>.  A pooling operator operates on individual feature channels, coalescing nearby feature values into one by the application of a suitable operator. Common choices include max-pooling (using the max operator) or sum-pooling (using summation). For example, <em>max-pooling</em> is defined as: <br>
<span class="MathJax_Preview">
   y_{ijk} = \max \{ y_{i'j'k} : i \leq i' &lt; i+p, j \leq j' &lt; j + p \}
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-48-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-48">
   y_{ijk} = \max \{ y_{i'j'k} : i \leq i' < i+p, j \leq j' < j + p \}
</script> <br>
Max pooling is implemented by the <code>vl_nnpool</code> function. Try this now:</p>



<pre class="prettyprint"><code class="language-matlab hljs ">y = vl_nnpool(x, <span class="hljs-number">15</span>) ;
figure(<span class="hljs-number">6</span>) ; clf ; imagesc(y) ;</code></pre>

<blockquote>
  <p><strong>Question:</strong> look at the resulting image. Can you interpret the result?</p>
</blockquote>

<p>The function <code>vl_nnpool</code> supports subsampling and padding just like <code>vl_nnconv</code>. However, for max-pooling feature maps are padded with the value <span class="MathJax_Preview">-\infty</span><span class="MathJax MathJax_Processing" id="MathJax-Element-49-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-49">-\infty</script> instead of 0. Why?</p>



<h3 id="part1-4" class="4">Part 1.4: normalisation</h3>

<p>Another important CNN building block is channel-wise normalisation. This operator normalises the vector of feature channels at each spatial location in the input map <span class="MathJax_Preview">\bx</span><span class="MathJax MathJax_Processing" id="MathJax-Element-50-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-50">\bx</script>. The form of the normalisation operator is actually rather curious: <br>
<span class="MathJax_Preview">
  y_{ijk'} = \frac{x_{ijk}}{\left(\kappa + \alpha \sum_{k\in G(k')}  x_{ijk}^2\right)^{\beta}}
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-51-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-51">
  y_{ijk'} = \frac{x_{ijk}}{\left(\kappa + \alpha \sum_{k\in G(k')}  x_{ijk}^2\right)^{\beta}}
</script> <br>
where <span class="MathJax_Preview">G(k) = \left[k - \lfloor \frac{\rho}{2} \rfloor, k + \lceil \frac{\rho}{2} \rceil\right] \cap \{1, 2, \dots, K\}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-52-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-52">G(k) = \left[k - \lfloor \frac{\rho}{2} \rfloor, k + \lceil \frac{\rho}{2} \rceil\right] \cap \{1, 2, \dots, K\}</script> is a group of <span class="MathJax_Preview">\rho</span><span class="MathJax MathJax_Processing" id="MathJax-Element-53-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-53">\rho</script> consecutive feature channels in the input map.</p>

<blockquote>
  <p><strong>Task:</strong> Understand what this operator is doing. How would you set <span class="MathJax_Preview">\kappa</span><span class="MathJax MathJax_Processing" id="MathJax-Element-54-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-54">\kappa</script>, <span class="MathJax_Preview">\alpha</span><span class="MathJax MathJax_Processing" id="MathJax-Element-55-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-55">\alpha</script> and <span class="MathJax_Preview">\beta</span><span class="MathJax MathJax_Processing" id="MathJax-Element-56-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-56">\beta</script> to achieve simple <span class="MathJax_Preview">L^2</span><span class="MathJax MathJax_Processing" id="MathJax-Element-57-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-57">L^2</script> normalisation?</p>
</blockquote>

<p>Now let’s try this out:</p>



<pre class="prettyprint"><code class="language-matlab hljs ">rho = <span class="hljs-number">5</span> ;
kappa = <span class="hljs-number">0</span> ;
alpha = <span class="hljs-number">1</span> ;
<span class="hljs-built_in">beta</span> = <span class="hljs-number">0.5</span> ;
y_nrm = vl_nnnormalize(x, <span class="hljs-matrix">[rho kappa alpha beta]</span>) ;
figure(<span class="hljs-number">6</span>) ; clf ; imagesc(y_nrm) ;</code></pre>

<blockquote>
  <p><strong>Tasks:</strong> </p>
  
  <ul>
  <li>Inspect the figure just obtained. Can you interpret it?</li>
  <li>Compute the <span class="MathJax_Preview">L^2</span><span class="MathJax MathJax_Processing" id="MathJax-Element-58-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-58">L^2</script> norm of the feature channels in the output map <code>y_nrm</code>. What do you notice? </li>
  <li>Explain this result in relation to the particular choice of the parameters <span class="MathJax_Preview">\rho</span><span class="MathJax MathJax_Processing" id="MathJax-Element-59-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-59">\rho</script>,  <span class="MathJax_Preview">\kappa</span><span class="MathJax MathJax_Processing" id="MathJax-Element-60-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-60">\kappa</script>, <span class="MathJax_Preview">\alpha</span><span class="MathJax MathJax_Processing" id="MathJax-Element-61-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-61">\alpha</script> and <span class="MathJax_Preview">\beta</span><span class="MathJax MathJax_Processing" id="MathJax-Element-62-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-62">\beta</script>.</li>
  </ul>
</blockquote>



<h2 id="part-2-back-propagation-and-derivatives">Part 2: back-propagation and derivatives</h2>

<p>The parameters of a CNN <span class="MathJax_Preview">\bw=(\bw_1,\dots\bw_L)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-63-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-63">\bw=(\bw_1,\dots\bw_L)</script> should be learned in such a manner that the overall CNN function  <span class="MathJax_Preview">\bz = f(\bx;\bw)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-64-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-64">\bz = f(\bx;\bw)</script> achieves a desired goal. In some cases,  the goal is to model the distribution of the data, which leads to a <em>generative objective</em>. Here, however, we will use <span class="MathJax_Preview">f</span><span class="MathJax MathJax_Processing" id="MathJax-Element-65-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-65">f</script> as a <em>regressor</em> and obtain it by minimising a <em>discriminative objective</em>. In simple terms, we are given:</p>

<ul>
<li>examples of the desired input-output relations <span class="MathJax_Preview">(\bx_1,\bz_1), \dots, (\bx_n,\bz_n)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-66-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-66">(\bx_1,\bz_1), \dots, (\bx_n,\bz_n)</script> where <span class="MathJax_Preview">\bx_i</span><span class="MathJax MathJax_Processing" id="MathJax-Element-67-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-67">\bx_i</script> are input data and <span class="MathJax_Preview">\bz_i</span><span class="MathJax MathJax_Processing" id="MathJax-Element-68-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-68">\bz_i</script> corresponding output labels;</li>
<li>and a loss <span class="MathJax_Preview">\ell(\bz,\hat\bz)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-69-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-69">\ell(\bz,\hat\bz)</script> that expresses the penalty for predicting <span class="MathJax_Preview">\hat\bz</span><span class="MathJax MathJax_Processing" id="MathJax-Element-70-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-70">\hat\bz</script> instead of <span class="MathJax_Preview">\bz</span><span class="MathJax MathJax_Processing" id="MathJax-Element-71-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-71">\bz</script>.</li>
</ul>

<p>We use those to write the empirical loss of the CNN <span class="MathJax_Preview">f</span><span class="MathJax MathJax_Processing" id="MathJax-Element-72-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-72">f</script> by averaging over the examples: <br>
<span class="MathJax_Preview">
L(\bw) = \frac{1}{n} \sum_{i=1}^n \ell(\bz_i, f(\bx_i;\bw))
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-73-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-73">
L(\bw) = \frac{1}{n} \sum_{i=1}^n \ell(\bz_i, f(\bx_i;\bw))
</script> <br>
Note that the composition of the function <span class="MathJax_Preview">f</span><span class="MathJax MathJax_Processing" id="MathJax-Element-74-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-74">f</script> with the loss <span class="MathJax_Preview">\ell</span><span class="MathJax MathJax_Processing" id="MathJax-Element-75-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-75">\ell</script> can be though of as a CNN with one more layer (called a <em>loss layer</em>). Hence, with a slight abuse of notation, in the rest of this part we incorporate the loss in the function <span class="MathJax_Preview">f</span><span class="MathJax MathJax_Processing" id="MathJax-Element-76-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-76">f</script> (which therefore is a map <span class="MathJax_Preview">\mathcal{X}\rightarrow\mathbb{R}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-77-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-77">\mathcal{X}\rightarrow\mathbb{R}</script>) and do not talk about it explicitly anymore.</p>

<p>The simplest algorithm to minimse <span class="MathJax_Preview">L</span><span class="MathJax MathJax_Processing" id="MathJax-Element-78-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-78">L</script>, and in fact one that is used in practice, is <em>gradient descent</em>. The idea is simple: compute the gradient of the objective <span class="MathJax_Preview">L</span><span class="MathJax MathJax_Processing" id="MathJax-Element-79-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-79">L</script> at a current solution <span class="MathJax_Preview">\bw^t</span><span class="MathJax MathJax_Processing" id="MathJax-Element-80-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-80">\bw^t</script> and then update the latter along the direction of fastest descent of <span class="MathJax_Preview">L</span><span class="MathJax MathJax_Processing" id="MathJax-Element-81-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-81">L</script>: <br>
<span class="MathJax_Preview">
 \bw^{t+1} = \bw^{t} - \eta_t \frac{\partial f}{\partial \bw}(\bw^t)
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-82-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-82">
 \bw^{t+1} = \bw^{t} - \eta_t \frac{\partial f}{\partial \bw}(\bw^t)
</script> <br>
where <span class="MathJax_Preview">\eta_t \in \mathbb{R}_+</span><span class="MathJax MathJax_Processing" id="MathJax-Element-83-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-83">\eta_t \in \mathbb{R}_+</script> is the <em>learning rate</em>.</p>



<h3 id="part-21-theory-of-backpropagation">Part 2.1: theory of backpropagation</h3>

<p>The basic computational problem to solve is the calculation of the gradient of the function with respect to the parameter <span class="MathJax_Preview">\bw</span><span class="MathJax MathJax_Processing" id="MathJax-Element-84-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-84">\bw</script>. Since <span class="MathJax_Preview">f</span><span class="MathJax MathJax_Processing" id="MathJax-Element-85-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-85">f</script> is the composition of several functions, the key ingredient is the <em>chain rule</em>: <br>
<span class="MathJax_Preview">
\begin{align}
 \frac{\partial f}{\partial \bw_l} &amp;= 
  \frac{\partial}{\partial \bw_l}
  f_L(\dots f_2(f_1(\bx;\bw_1);\bw_2)\dots),\bw_{L})
  \\
  &amp;= 
  \frac{\partial \vc f_L}{\partial \vc\bx_L^\top}
  \frac{\partial \vc f_{L-1}}{\partial \vc\bx_{L-1}^\top}
  \dots
  \frac{\partial \vc f_{l+1}}{\partial \vc\bx_{l+1}^\top}
  \frac{\partial \vc f_l}{\partial \bw_l^\top}
\end{align}
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-86-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-86">
\begin{align}
 \frac{\partial f}{\partial \bw_l} &= 
  \frac{\partial}{\partial \bw_l}
  f_L(\dots f_2(f_1(\bx;\bw_1);\bw_2)\dots),\bw_{L})
  \\
  &= 
  \frac{\partial \vc f_L}{\partial \vc\bx_L^\top}
  \frac{\partial \vc f_{L-1}}{\partial \vc\bx_{L-1}^\top}
  \dots
  \frac{\partial \vc f_{l+1}}{\partial \vc\bx_{l+1}^\top}
  \frac{\partial \vc f_l}{\partial \bw_l^\top}
\end{align}
</script> <br>
The notation requires some explanation. Recall that each function <span class="MathJax_Preview">f_l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-87-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-87">f_l</script> is a map from a <span class="MathJax_Preview">M\times N\times K</span><span class="MathJax MathJax_Processing" id="MathJax-Element-88-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-88">M\times N\times K</script> array to a <span class="MathJax_Preview">M' \times N' \times K'</span><span class="MathJax MathJax_Processing" id="MathJax-Element-89-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-89">M' \times N' \times K'</script> array. The operator <span class="MathJax_Preview">\vc</span><span class="MathJax MathJax_Processing" id="MathJax-Element-90-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-90">\vc</script> <em>vectorises</em> such arrays by stacking their elments in a column vector (the stacking order is arbitrary but conventionally column-major). The symbol <span class="MathJax_Preview">\partial \vc f_l / \partial \vc \bx_l^\top</span><span class="MathJax MathJax_Processing" id="MathJax-Element-91-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-91">\partial \vc f_l / \partial \vc \bx_l^\top</script> then denotes the derivative of a column vector of output variables by a row vector of input variables. Note that <span class="MathJax_Preview">\bw_l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-92-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-92">\bw_l</script> is already assumed to be a column vector so it does not require explicit vectorisation.</p>

<blockquote>
  <p><strong>Questions:</strong> Make sure you understand the structure of this formula and answer the following:</p>
  
  <ul>
  <li><span class="MathJax_Preview">\partial \vc f_l / \partial \vc \bx_l^\top</span><span class="MathJax MathJax_Processing" id="MathJax-Element-93-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-93">\partial \vc f_l / \partial \vc \bx_l^\top</script> is a matrix. What are its dimensions?</li>
  <li>The formula can be rewritten with a slightly different notation by replacing the symbols <span class="MathJax_Preview">f_l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-94-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-94">f_l</script> with the symbols <span class="MathJax_Preview">\bx_{l+1}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-95-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-95">\bx_{l+1}</script>. If you do so, do you notice any formal cancelation?</li>
  <li>The formula only includes the derivative symbols. However, these derivatives must be computed at a well defined point. What is this point?</li>
  </ul>
</blockquote>

<p>To apply the chain rule we must be able to compute, for each function <span class="MathJax_Preview">l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-96-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-96">l</script>, its derivative with respect to the parameters <span class="MathJax_Preview">\bw_l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-97-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-97">\bw_l</script> as well as its input <span class="MathJax_Preview">\bx_l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-98-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-98">\bx_l</script>. While this could be done naively, a problem is the very high dimensionality of the matrices involved in this calculation as these are <span class="MathJax_Preview">M'N'K' \times MNK</span><span class="MathJax MathJax_Processing" id="MathJax-Element-99-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-99">M'N'K' \times MNK</script> arrays. We will now introduce a “trick” that allows reducing this to working with <span class="MathJax_Preview">MNK</span><span class="MathJax MathJax_Processing" id="MathJax-Element-100-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-100">MNK</script> numbers only and which will yield to the <em>backpropagation algorithm</em>. </p>

<p>The key observation is that we ear not after <span class="MathJax_Preview">\partial \vc f_l / \partial \bw_l^\top</span><span class="MathJax MathJax_Processing" id="MathJax-Element-101-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-101">\partial \vc f_l / \partial \bw_l^\top</script> but after <span class="MathJax_Preview">\partial f/ \partial\bw_l^\top</span><span class="MathJax MathJax_Processing" id="MathJax-Element-102-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-102">\partial f/ \partial\bw_l^\top</script>: <br>
<span class="MathJax_Preview">
 \frac{\partial f}{\partial\bw_l^\top}
 =
 \frac{\partial g_{l+1}}{\partial \vc \bx_{l+1}^\top}
  \frac{\partial \vc f_l}{\partial \bw_l^\top}
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-103-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-103">
 \frac{\partial f}{\partial\bw_l^\top}
 =
 \frac{\partial g_{l+1}}{\partial \vc \bx_{l+1}^\top}
  \frac{\partial \vc f_l}{\partial \bw_l^\top}
</script> <br>
where <span class="MathJax_Preview">g_{l+1} = f_L \circ \dots \circ f_{l+1}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-104-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-104">g_{l+1} = f_L \circ \dots \circ f_{l+1}</script> is the “tail” of the CNN.</p>

<blockquote>
  <p><strong>Question:</strong> Explain why the diemnsions of the vectors <span class="MathJax_Preview">\partial g_{l+1}/\partial \vc \bx_{l+1}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-105-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-105">\partial g_{l+1}/\partial \vc \bx_{l+1}</script> and <span class="MathJax_Preview">\partial f/\partial \bw_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-106-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-106">\partial f/\partial \bw_{l}</script> equals the number of elements in <span class="MathJax_Preview">\bx_{l+1}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-107-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-107">\bx_{l+1}</script> and <span class="MathJax_Preview">\bw_l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-108-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-108">\bw_l</script> respectively. Hence, in particular, the symbol <span class="MathJax_Preview">\partial g_{l+1}/\partial \bx_{l+1}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-109-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-109">\partial g_{l+1}/\partial \bx_{l+1}</script> (without vectorisation) denotes an array with the same size of <span class="MathJax_Preview">\bx_{l+1}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-110-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-110">\bx_{l+1}</script>.</p>
  
  <p><strong>Hint:</strong> recall that the last layer is the loss.</p>
</blockquote>

<p>Hence the algorithm can focus on computing the derivatives of <span class="MathJax_Preview">g_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-111-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-111">g_{l}</script> instead of <span class="MathJax_Preview">f_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-112-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-112">f_{l}</script> which are far lower-dimensional. To see how this can be done iteratively, decompose <span class="MathJax_Preview">g_l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-113-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-113">g_l</script> as: <br>
<span class="MathJax_Preview">
 \bx_{l}
 \longrightarrow 
 \underset{\displaystyle\underset{\displaystyle\bw_l}{\uparrow}}{\boxed{f_l}} 
 \longrightarrow
 \bx_{l+1}  
 \longrightarrow 
 \boxed{g_{l+1}}
 \longrightarrow
 \bx_{L}
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-114-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-114">
 \bx_{l}
 \longrightarrow 
 \underset{\displaystyle\underset{\displaystyle\bw_l}{\uparrow}}{\boxed{f_l}} 
 \longrightarrow
 \bx_{l+1}  
 \longrightarrow 
 \boxed{g_{l+1}}
 \longrightarrow
 \bx_{L}
</script> <br>
Then the key of the iteration is obtaining the derivatives for layer <span class="MathJax_Preview">l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-115-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-115">l</script> given the ones for layer <span class="MathJax_Preview">l+1</span><span class="MathJax MathJax_Processing" id="MathJax-Element-116-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-116">l+1</script>:</p>

<ul>
<li>Input: <br>
<ul><li>the derivative <span class="MathJax_Preview">\partial g_{l+1}/\partial \bx_{l+1}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-117-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-117">\partial g_{l+1}/\partial \bx_{l+1}</script>.</li></ul></li>
<li>Output: <br>
<ul><li>the derivative <span class="MathJax_Preview">\partial g_{l}/\partial \bx_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-118-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-118">\partial g_{l}/\partial \bx_{l}</script></li>
<li>the derivative  <span class="MathJax_Preview">\partial g_{l}/\partial \bw_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-119-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-119">\partial g_{l}/\partial \bw_{l}</script></li></ul></li>
</ul>

<blockquote>
  <p><strong>Question:</strong> Suppose that <span class="MathJax_Preview">f_l</span><span class="MathJax MathJax_Processing" id="MathJax-Element-120-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-120">f_l</script> is the function <span class="MathJax_Preview">\bx_{l+1} = A \bx_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-121-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-121">\bx_{l+1} = A \bx_{l}</script> where <span class="MathJax_Preview">\bx_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-122-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-122">\bx_{l}</script> and <span class="MathJax_Preview">\bx_{l+1}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-123-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-123">\bx_{l+1}</script> are column vectors. Suppose that <span class="MathJax_Preview">B = \partial g_{l+1}/\partial \bx_{l+1} </span><span class="MathJax MathJax_Processing" id="MathJax-Element-124-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-124">B = \partial g_{l+1}/\partial \bx_{l+1} </script> is given. Derive an expression for <span class="MathJax_Preview">C = \partial g_{l}/\partial \bx_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-125-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-125">C = \partial g_{l}/\partial \bx_{l}</script> and an expression for <span class="MathJax_Preview">D =  \partial g_{l}/\partial \bw_{l}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-126-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-126">D =  \partial g_{l}/\partial \bw_{l}</script>.</p>
</blockquote>



<h3 id="part-21-putting-backpropagation-to-the-practice">Part 2.1: putting backpropagation to the practice</h3>

<p>A key feature of MatConvNet and similar neural-netowrk packages is the ability to support back-propagation. In order to do so, lets focus on a single computational block <span class="MathJax_Preview">f</span><span class="MathJax MathJax_Processing" id="MathJax-Element-127-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-127">f</script>, followed by a function <span class="MathJax_Preview">g</span><span class="MathJax MathJax_Processing" id="MathJax-Element-128-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-128">g</script>: <br>
<span class="MathJax_Preview">
 \bx
 \longrightarrow 
 \underset{\displaystyle\underset{\displaystyle\bw}{\uparrow}}{\boxed{f}} 
 \longrightarrow
 \by
 \longrightarrow 
 \boxed{g}
 \longrightarrow
 z
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-129-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-129">
 \bx
 \longrightarrow 
 \underset{\displaystyle\underset{\displaystyle\bw}{\uparrow}}{\boxed{f}} 
 \longrightarrow
 \by
 \longrightarrow 
 \boxed{g}
 \longrightarrow
 z
</script> <br>
where <em><span class="MathJax_Preview">z</span><span class="MathJax MathJax_Processing" id="MathJax-Element-130-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-130">z</script> is assumed to be a scalar</em>. Then each computation block (for example <code>vl_nnconv</code> or <code>vl_nnpool</code>) can compute <span class="MathJax_Preview">\partial z / \partial \bx</span><span class="MathJax MathJax_Processing" id="MathJax-Element-131-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-131">\partial z / \partial \bx</script> and <span class="MathJax_Preview">\partial z / \partial \bw</span><span class="MathJax MathJax_Processing" id="MathJax-Element-132-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-132">\partial z / \partial \bw</script> given as input <span class="MathJax_Preview">\bx</span><span class="MathJax MathJax_Processing" id="MathJax-Element-133-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-133">\bx</script> and <span class="MathJax_Preview">\partial z / \partial \by</span><span class="MathJax MathJax_Processing" id="MathJax-Element-134-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-134">\partial z / \partial \by</script>. Let’s put this to the practice:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Read an example image</span>
x = im2single(imread(<span class="hljs-string">'peppers.png'</span>)) ;

<span class="hljs-comment">% Create a bank of linear filters and apply them to the image</span>
w = <span class="hljs-built_in">randn</span>(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,<span class="hljs-number">10</span>,<span class="hljs-string">'single'</span>) ;
y = vl_nnconv(x, w, <span class="hljs-matrix">[]</span>) ;

<span class="hljs-comment">% Create the derivative dz/dy</span>
dzdy = <span class="hljs-built_in">randn</span>(<span class="hljs-built_in">size</span>(y), <span class="hljs-string">'single'</span>) ;

<span class="hljs-comment">% Back-propagation</span>
<span class="hljs-matrix">[dzdx, dzdw]</span> = vl_nnconv(x, w, <span class="hljs-matrix">[]</span>, dzdy) ;</code></pre>

<blockquote>
  <p><strong>Task:</strong> Run the code above and check the dimensions of <code>dzdx</code> and <code>dzdy</code>. Does this matches your expectations?</p>
</blockquote>

<p>An advantage of this modular view is that new building blocks can be coded and added to the architecture in a simple manner. However, it is easy to make mistakes in the calculation of complex derivatives. Hence, it is a good idea to verify results numerically. Consider the following piece of code:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Check the derivative numerically</span>
ex = <span class="hljs-built_in">randn</span>(<span class="hljs-built_in">size</span>(x), <span class="hljs-string">'single'</span>) ;
eta = <span class="hljs-number">0.0001</span> ;
xp = x + eta * ex  ;
yp = vl_nnconv(xp, w, <span class="hljs-matrix">[]</span>) ;

dzdx_empirical = sum(dzdy(:) .* (yp(:) - y(:)) / eta) ;
dzdx_computed = sum(dzdx(:) .* ex(:)) ;

fprintf(...
  <span class="hljs-string">'der: empirical: %f, computed: %f, error: %.2f %%\n'</span>, ...
  dzdx_empirical, dzdx_computed, ...
  <span class="hljs-built_in">abs</span>(<span class="hljs-number">1</span> - dzdx_empirical/dzdx_computed)*<span class="hljs-number">100</span>) ;</code></pre>

<blockquote>
  <p><strong>Questions:</strong></p>
  
  <ul>
  <li>What is the meaning f <code>ex</code> in the code above? </li>
  <li>What are the derivatives <code>dzdx_empirical</code> and <code>dzdx_comptued</code>?</li>
  </ul>
  
  <p><strong>Tasks:</strong></p>
  
  <ul>
  <li>Run the code and convince yourself that <code>vl_nnconv</code> derivatives is (probably) correct.</li>
  <li>Create a new version of this code to test the derivative calculation with respect to <span class="MathJax_Preview">\bw</span><span class="MathJax MathJax_Processing" id="MathJax-Element-135-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-135">\bw</script>.</li>
  </ul>
</blockquote>

<p>We are now ready to build our first elementary CNN, composed of just two layers, and to compute its derivatives:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Parameters of the CNN</span>
w1 = <span class="hljs-built_in">randn</span>(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,<span class="hljs-number">10</span>,<span class="hljs-string">'single'</span>) ;
rho2 = <span class="hljs-number">10</span> ;

<span class="hljs-comment">% Run the CNN forward</span>
x1 = im2single(imread(<span class="hljs-string">'peppers.png'</span>)) ;
x2 = vl_nnconv(x1, w1, <span class="hljs-matrix">[]</span>) ;
x3 = vl_nnpool(x2, rho2) ;

<span class="hljs-comment">% Create the derivative dz/dx3</span>
dzdx3 = <span class="hljs-built_in">randn</span>(<span class="hljs-built_in">size</span>(x3), <span class="hljs-string">'single'</span>) ;

<span class="hljs-comment">% Run the CNN backward</span>
dzdx2 = vl_nnpool(x2, rho2, dzdx3) ;
<span class="hljs-matrix">[dzdx1, dzdw1]</span> = vl_nnconv(x1, w1, <span class="hljs-matrix">[]</span>, dzdx2) ;</code></pre>

<blockquote>
  <p><strong>Question:</strong> Note that the last derivative in the CNN is <code>dzdx3</code>. Here, for the sake of the example, this derivative is initalized randomly. In a practical application, what would this derivative represent?</p>
</blockquote>

<p>We can now use the same technique as before to check that the derivative computed through back-propagation are correct.</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Check the derivative numerically</span>
ew1 = <span class="hljs-built_in">randn</span>(<span class="hljs-built_in">size</span>(w1), <span class="hljs-string">'single'</span>) ;
eta = <span class="hljs-number">0.0001</span> ;
w1p = w1 + eta * ew1  ;

x1p = x1 ;
x2p = vl_nnconv(x1p, w1p, <span class="hljs-matrix">[]</span>) ;
x3p = vl_nnpool(x2p, rho2) ;

dzdw1_empirical = sum(dzdx3(:) .* (x3p(:) - x3(:)) / eta) ;
dzdw1_computed = sum(dzdw1(:) .* ew1(:)) ;

fprintf(...
  <span class="hljs-string">'der: empirical: %f, computed: %f, error: %.2f %%\n'</span>, ...
  dzdw1_empirical, dzdw1_computed, ...
  <span class="hljs-built_in">abs</span>(<span class="hljs-number">1</span> - dzdw1_empirical/dzdw1_computed)*<span class="hljs-number">100</span>) ;</code></pre>



<h2 id="part-3-learning-a-tiny-cnn">Part 3: learning a tiny CNN</h2>

<p>In this part we will learn a very simple CNN. The CNN is composed of exactly two layers: a convolutional layear and a max-pooling layer: <br>
<span class="MathJax_Preview">
\bx_2 = W * \bx_1 + b, \qquad \bx_3 = \operatorname{maxpool}_\rho \bx_2.
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-136-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-136">
\bx_2 = W * \bx_1 + b, \qquad \bx_3 = \operatorname{maxpool}_\rho \bx_2.
</script> <br>
<span class="MathJax_Preview">W</span><span class="MathJax MathJax_Processing" id="MathJax-Element-137-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-137">W</script> contains a single <span class="MathJax_Preview">3\times 3</span><span class="MathJax MathJax_Processing" id="MathJax-Element-138-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-138">3\times 3</script> square filter, so that <span class="MathJax_Preview">b</span><span class="MathJax MathJax_Processing" id="MathJax-Element-139-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-139">b</script> is a scalar. and the input image <span class="MathJax_Preview">\bx=\bx_1</span><span class="MathJax MathJax_Processing" id="MathJax-Element-140-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-140">\bx=\bx_1</script> has a single channel.</p>

<blockquote>
  <p><strong>Task</strong></p>
  
  <ul>
  <li>Open the file <code>tinycnn.m</code> and inspect the code. Convince yourself that the code computes the CNN just described.</li>
  <li>Look at the paddings used in the code. If the input image <span class="MathJax_Preview">\bx_1</span><span class="MathJax MathJax_Processing" id="MathJax-Element-141-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-141">\bx_1</script> has dimensions <span class="MathJax_Preview">M\times N</span><span class="MathJax MathJax_Processing" id="MathJax-Element-142-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-142">M\times N</script>, what is the dimension of the output feature map <span class="MathJax_Preview">\bx_3</span><span class="MathJax MathJax_Processing" id="MathJax-Element-143-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-143">\bx_3</script>?</li>
  </ul>
</blockquote>

<p>In the rest of the section we will learn the CNN parameters in order to extract blob-like structures from images, such as the ones in the following image:</p>

<p><img width="350px" src="images/dots.jpg" alt="sentence-lato"></p>



<h3 id="part-31-training-data-and-labels">Part 3.1: training data and labels</h3>

<p>The first step is to load the image <code>data/dots.jpg</code> and to use the supplied <code>extractBlackBlobs</code> function to extract all the black dots in the image.</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Load an image</span>
im = rgb2gray(im2single(imread(<span class="hljs-string">'data/dots.jpg'</span>))) ;

<span class="hljs-comment">% Compute the location of black blobs in the image</span>
<span class="hljs-matrix">[pos,neg]</span> = extractBlackBlobs(im) ;</code></pre>

<p>The arrays <code>pos</code> and <code>neg</code> contain now pixel labels and  will be used as <em>annotations</em> for the supervised training of the CNN. These annotations can be visualised as follows:</p>



<pre class="prettyprint"><code class="language-matlab hljs ">figure(<span class="hljs-number">1</span>) ; clf ; 
subplot(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>) ; imagesc(im) ; axis equal ; title(<span class="hljs-string">'image'</span>) ;
subplot(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>) ; imagesc(pos) ; axis equal ; title(<span class="hljs-string">'positive points (blob centres)'</span>) ;
subplot(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>) ; imagesc(neg) ; axis equal ; title(<span class="hljs-string">'negative points (not a blob)'</span>) ;
colormap gray ; </code></pre>

<blockquote>
  <p><strong>Task:</strong> Inspect <code>pos</code> and <code>neg</code> and convince yourself that:</p>
  
  <ul>
  <li><code>pos</code> contains a single <code>true</code> value in correspondence of each blob centre;</li>
  <li><code>neg</code> contains a <code>true</code> value for each pixel sufficiently far away from a blob.</li>
  </ul>
  
  <p>Are there pixels for which both <code>pos</code> and <code>neg</code> evaluate to false?</p>
</blockquote>



<h3 id="part-32-image-preprocessing">Part 3.2: image preprocessing</h3>

<p>Before we attempt to train the CNN, the image is pre-processed to remove its mean value. It is also smoothed by applying a Gaussian kernel of standard deviation 3 pixels:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Pre-smooth the image</span>
im = vl_imsmooth(im,<span class="hljs-number">3</span>) ;

<span class="hljs-comment">% Subtract median value</span>
im = im - median(im(:)) ;</code></pre>

<p>We will come back to this preprocessing steps later.</p>



<h3 id="part-33-learning-with-gradient-descent">Part 3.3: learning with gradient descent</h3>

<p>We will now setup a learning problem to learn <span class="MathJax_Preview">W</span><span class="MathJax MathJax_Processing" id="MathJax-Element-144-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-144">W</script> and <span class="MathJax_Preview">b</span><span class="MathJax MathJax_Processing" id="MathJax-Element-145-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-145">b</script> to detect black blobs in images. Recall that the CNN computes for each image pixel <span class="MathJax_Preview">(u,v)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-146-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-146">(u,v)</script> a score <span class="MathJax_Preview">f(\bx;\bw,b)_{(u,v)}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-147-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-147">f(\bx;\bw,b)_{(u,v)}</script>. We would like this score to be:</p>

<ul>
<li>at least as large as 1 for any pixel that is marked as a blob centre (<code>pos</code> or <span class="MathJax_Preview">(u,v)\in\cP</span><span class="MathJax MathJax_Processing" id="MathJax-Element-148-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-148">(u,v)\in\cP</script>) and</li>
<li>at most zero for any pixel that is marked as being far away from a blob (<code>neg</code> or <span class="MathJax_Preview">(u,v)\in\cN</span><span class="MathJax MathJax_Processing" id="MathJax-Element-149-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-149">(u,v)\in\cN</script>).</li>
</ul>

<p>We do so by defining and then optimising the following objective function: <br>
<span class="MathJax_Preview">
E(\bw,b) = 
\frac{\lambda}{2}\|\bw\|^2
+
\frac{1}{|\cP|}\sum_{(u,v) \in \cP}
\max\{0, 1 - f(\bx;\bw,b)_{(u,v)}\}
+
\frac{1}{|\cN|}\sum_{(u,v) \in \cN}
\max\{0, f(\bx;\bw,b)_{(u,v)}\}.
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-150-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-150">
E(\bw,b) = 
\frac{\lambda}{2}\|\bw\|^2
+
\frac{1}{|\cP|}\sum_{(u,v) \in \cP}
\max\{0, 1 - f(\bx;\bw,b)_{(u,v)}\}
+
\frac{1}{|\cN|}\sum_{(u,v) \in \cN}
\max\{0, f(\bx;\bw,b)_{(u,v)}\}.
</script></p>

<blockquote>
  <p><strong>Questions:</strong></p>
  
  <ul>
  <li>What can you say about the score of each pixel if <span class="MathJax_Preview">\lambda=0</span><span class="MathJax MathJax_Processing" id="MathJax-Element-151-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-151">\lambda=0</script> and <span class="MathJax_Preview">E(\bw,b) =0</span><span class="MathJax MathJax_Processing" id="MathJax-Element-152-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-152">E(\bw,b) =0</script>?</li>
  <li>Note that the objective enforces a <em>marging</em> between the scores of the positive and negative pixels. How much is this margin?</li>
  </ul>
</blockquote>

<p>We can now train the CNN by minimising the objective function with respect to <span class="MathJax_Preview">\bw</span><span class="MathJax MathJax_Processing" id="MathJax-Element-153-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-153">\bw</script> and <span class="MathJax_Preview">b</span><span class="MathJax MathJax_Processing" id="MathJax-Element-154-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-154">b</script>. We do so by using an algorithm called <em>gradient descent with momentum</em>.  Given the current solution <span class="MathJax_Preview">(\bw_t,b_t)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-155-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-155">(\bw_t,b_t)</script> and update it , this is updated to <span class="MathJax_Preview">(\bw_{t+1},b_t)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-156-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-156">(\bw_{t+1},b_t)</script> by following the direction of fastest descent as given by the negative gradient <span class="MathJax_Preview">-\nabla E(\bw_t,b_t)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-157-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-157">-\nabla E(\bw_t,b_t)</script> of the objective. However, gradient updates are smoothed by considering a <em>momentum</em> term <span class="MathJax_Preview">(\bar\bw_{t}, \bar\mu_t)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-158-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-158">(\bar\bw_{t}, \bar\mu_t)</script>, yielding the update equations <br>
<span class="MathJax_Preview">
 \bar\bw_{t+1} \leftarrow \mu \bar\bw_t + \eta \frac{\partial E}{\partial \bw_t},
 \qquad
 \bw_{t+1} \leftarrow \bw_{t} - \bar\bw_t.
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-159-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-159">
 \bar\bw_{t+1} \leftarrow \mu \bar\bw_t + \eta \frac{\partial E}{\partial \bw_t},
 \qquad
 \bw_{t+1} \leftarrow \bw_{t} - \bar\bw_t.
</script> <br>
and similarly for the bias term. Here <span class="MathJax_Preview">\mu</span><span class="MathJax MathJax_Processing" id="MathJax-Element-160-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-160">\mu</script> is the <em>momentum rate</em> and <span class="MathJax_Preview">\eta</span><span class="MathJax MathJax_Processing" id="MathJax-Element-161-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-161">\eta</script> the <em>learning rate</em>.</p>

<blockquote>
  <p><strong>Questions:</strong></p>
  
  <ul>
  <li>Explain why the momentum rate must be smaller than 1. What is the effect of having a momentum rate close to 1?</li>
  <li>The learning rate establishes how fast the algorithm will try to minimise the objective function. Can you see any problem with a large learning rate?</li>
  </ul>
</blockquote>

<p>The parameters of the algorithm are set as follows:</p>



<pre class="prettyprint"><code class="language-Malta hljs ini"><span class="hljs-setting">numIterations = <span class="hljs-value"><span class="hljs-number">500</span> ;</span></span>
<span class="hljs-setting">rate = <span class="hljs-value"><span class="hljs-number">5</span> ;</span></span>
<span class="hljs-setting">momentum = <span class="hljs-value"><span class="hljs-number">0.9</span> ;</span></span>
<span class="hljs-setting">shrinkRate = <span class="hljs-value"><span class="hljs-number">0.0001</span> ;</span></span>
<span class="hljs-setting">plotPeriod = <span class="hljs-value"><span class="hljs-number">10</span> ;</span></span></code></pre>

<blockquote>
  <p><strong>Tasks:</strong></p>
  
  <ul>
  <li>Inspect the code in the file  <code>exercise3.m</code>. Convince yourself that the code is implementing the algorithm described above. Pay particular attention at the forward and backward passes as well as at how the objective function and its derivatives are computed.</li>
  <li>Run the algorithm and observe the results. Then answer the following questions: <br>
  <ul><li>The learned filter should resemble the discretisation of a well-known differential operator. Which one? </li>
  <li>What is the average of the filter values compared to the average of the absolute values?</li></ul></li>
  <li>Run the algorithm again and observe the evolution of the histograms of the score of the positive and negative pixels in relation to the values 0 and 1. Answer the following: <br>
  <ul><li>Is the objective function minimised monotonically?</li>
  <li>As the histograms evolve, can you identify at least two “phases” in the optimisation?</li>
  <li>Once converged, do the score distribute in the manner that you would expect?</li></ul></li>
  </ul>
  
  <p><strong>Hint:</strong> the <code>plotPeriod</code> option can be changed to plot the diagnostic figure with a higher or lower frequency; this can singificantly affect the speed of the algorithm.</p>
</blockquote>



<h3 id="part-34-experimenting-with-the-tiny-cnn">Part 3.4: experimenting with the tiny CNN</h3>

<p>In this part we will experiment with several variants of the network just learned. First, we study the effect of the image smoothing:</p>

<blockquote>
  <p><strong>Task:</strong> Train again the tiny CNN <em>without smoothing the input image in prerprocessing</em>. Anwser the following questions:</p>
  
  <ul>
  <li>Is the learned filter very different from the one learned before?</li>
  <li>If so, can you figure out what “went wrong”?</li>
  <li>Look carefully at the output of the first layer, magnifying with th loupe tool. Is the maximal filter response attained in the middle of each blob?</li>
  </ul>
  
  <p><strong>Hint:</strong> The Laplacian of Gaussian operator responds maximally at the centre of a blob only if the latter matches the blob size. Relate this fact to the combination of pre-smoothing the image and applying the learned <span class="MathJax_Preview">3\times 3</span><span class="MathJax MathJax_Processing" id="MathJax-Element-162-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-162">3\times 3</script> filter.</p>
</blockquote>

<p>Now restore the smoothing but switch off subtracting the median from the input image.</p>

<blockquote>
  <p><strong>Task:</strong> Train again the tiny CNN <em>without subtracting the median value in prerprocessing</em>. Answer the following questions:</p>
  
  <ul>
  <li>Does the algorithm converge?</li>
  <li>Reduce a hundred-fold the learning are and increase the maximum number of iterations by an equal amount. Does it get better?</li>
  <li>Explain why adding a constant to the input image can have such a dramatic effect on the performance of the optimisation.</li>
  </ul>
  
  <p><strong>Hint:</strong> What constraint should the filter <span class="MathJax_Preview">\bw</span><span class="MathJax MathJax_Processing" id="MathJax-Element-163-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-163">\bw</script> satisfy if the filter output should be zero when (i) the input image is zero or (ii) the input image is a large constant? Do you think that ti would be easy for gradient descent to enforce (ii) at all times?</p>
</blockquote>

<p>What you have just witnessed is actually a fairly general principle: centering the data usually makes learning problems much better conditioned.</p>

<p>Now we will explore several parameters in the algorithms:</p>

<blockquote>
  <p><strong>Task:</strong> Restore the preprocessing as given in <code>experiment4.m</code>.  Try the following:</p>
  
  <ul>
  <li>Try increasing th learning rate <code>eta</code>. Can you achieve a better value of the energy in the 500 iterations?</li>
  <li>Disable momentum by setting <code>momentum = 0</code>. Now try to beat the result obtained above by choosing <code>eta</code>. Can you succeed?</li>
  </ul>
</blockquote>

<p>Finally, consider the regularization effect of shrinking:</p>

<blockquote>
  <p><strong>Task:</strong> Restore the learning rate and momentum as given in <code>experiment4.m</code>. Then increase the shrkincage factor tenfold and a hundred-fold.</p>
  
  <ul>
  <li>What is the effect on the convergence speed?</li>
  <li>What is the effect on the final value of the total objective function and of the average loss part of it?</li>
  </ul>
</blockquote>



<h2 id="part-4-learning-a-character-cnn">Part 4: learning a character CNN</h2>

<p>In this part we will learn a CNN to recognize images of characters.  </p>



<h3 id="part-41-prepare-the-data">Part 4.1: prepare the data</h3>

<p>Open up <code>exercise4.m</code> and execute Part 4.1. The code loads a structure <code>imdb</code> containing images of the characters <em>a, b, …, z</em> rendered using approximately 931 fonts downloaded from the <a href="https://www.google.com/fonts">Google Fonts Project</a>. Look at the <code>imdb.images</code> substructure:</p>



<pre class="prettyprint"><code class="language-matlab hljs ">&gt;&gt; <span class="hljs-transposed_variable">imdb.</span>images
<span class="hljs-built_in">ans</span> = 
       id: <span class="hljs-matrix">[<span class="hljs-number">1</span>x24206 double]</span>
     data: <span class="hljs-matrix">[<span class="hljs-number">32</span>x32x24206 single]</span>
    label: <span class="hljs-matrix">[<span class="hljs-number">1</span>x24206 double]</span>
      set: <span class="hljs-matrix">[<span class="hljs-number">1</span>x24206 double]</span></code></pre>

<p>These are stored as the array <code>imdb.images.id</code> is a 24,206-dimensional vector of numeric IDs for each of the 24,206 character images in the dataset. <code>imdb.images.data</code> contains a <span class="MathJax_Preview">32 \times 32</span><span class="MathJax MathJax_Processing" id="MathJax-Element-164-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-164">32 \times 32</script> image for each character, stored as a slide of a <span class="MathJax_Preview">32\times 32\times 24,\!206</span><span class="MathJax MathJax_Processing" id="MathJax-Element-165-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-165">32\times 32\times 24,\!206</script>-dimesnional array. <code>imdb.images.label</code> is a vector of image labels, denoting which one of the 26 possible characters it is. <code>imdb.images.set</code> is equal to 1 for each image that should be used to train the CNN and to 2 for each image that should be used for validation.</p>

<p><img height="400px" src="images/chars.png" alt="cover"></p>

<blockquote>
  <p><strong>Task:</strong> look at the Figure 1 generated by the code and at the code itself and make sure that you understand what you are looking at.</p>
</blockquote>



<h3 id="part-42-intialize-a-cnn-architecture">Part 4.2: intialize a CNN architecture</h3>

<p>The function <code>initializeCharacterCNN.m</code> creates a CNN initialised with random weights that will be trained to recognise character images.</p>

<blockquote>
  <p><strong>Tasks:</strong></p>
  
  <ol>
  <li>By inspecting <code>initializeCharacterCNN.m</code> get a sense of the architecture that will be trained. How many layers are there? How big are the filters?</li>
  <li>Use the function <a href="http://www.vlfeat.org/matconvnet/mfiles/vl_simplenn_display/"><code>vl_simplenn_display</code></a> to produce a table summarising the architecture.</li>
  </ol>
</blockquote>

<p>Note that the <em>penultimate</em> layer has 26 output dimensions, one for each character. Character recognition looks at the maximal output to identify which character is processed by the network.</p>

<p>However, the last network layer is <a href="http://www.vlfeat.org/matconvnet/mfiles/vl_nnsoftmaxloss/"><code>vl_nnsoftmaxloss</code></a>, which in turn is a combination of the <a href="http://www.vlfeat.org/matconvnet/mfiles/vl_nnsoftmax/"><code>vl_nnsoftmax</code></a> function and of the classification log-loss <a href="http://www.vlfeat.org/matconvnet/mfiles/vl_nnloss/"><code>vl_nnloss</code></a>. The <em>softmax</em> operator is given by <br>
<span class="MathJax_Preview">
  y_{ijk'} = \frac{e^{x_{ijk'}}}{\sum_{k} e^{x_{ijk}}}
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-166-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-166">
  y_{ijk'} = \frac{e^{x_{ijk'}}}{\sum_{k} e^{x_{ijk}}}
</script> <br>
whereas the <em>log-loss</em> is given by <br>
<span class="MathJax_Preview">
  y_{ij} = - \log x_{ij c_{ij}}
</span><div class="MathJax_Display MathJax_Processing" role="textbox" aria-readonly="true"><span class="MathJax" id="MathJax-Element-167-Frame"></span></div><script type="math/tex; mode=display" id="MathJax-Element-167">
  y_{ij} = - \log x_{ij c_{ij}}
</script> <br>
where <span class="MathJax_Preview">c_{ij}</span><span class="MathJax MathJax_Processing" id="MathJax-Element-168-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-168">c_{ij}</script> is the index of the ground-truth class at spatial location <span class="MathJax_Preview">(i,j)</span><span class="MathJax MathJax_Processing" id="MathJax-Element-169-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-169">(i,j)</script>.</p>

<p><strong>Remark:</strong> While in MatConvNet all operators are convolutional, in this case the network is configured such that the output of the classification layer is a <span class="MathJax_Preview">1 \times 1 \times 26</span><span class="MathJax MathJax_Processing" id="MathJax-Element-170-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-170">1 \times 1 \times 26</script>-dimensional feature map, i.e. there remains only one spatial location.</p>

<blockquote>
  <p><strong>Tasks:</strong></p>
  
  <ol>
  <li>Understand what the softmax operator does. <strong>Hint:</strong> to use the log-loss the data must me in the (0, 1] interval.</li>
  <li>Understand what is the effect of minimising the log-loss. Which neural response should become larger?</li>
  <li>Why do you think MatConvNet provides a third function <code>vl_nnsoftmaxloss</code> combining both functions into a single layer?</li>
  </ol>
</blockquote>



<h3 id="part-43-train-and-evaluate-the-cnn">Part 4.3: train and evaluate the CNN</h3>

<p>We are now ready to train the CNN. To this end we use the example SGD implementation in MatConvNet (<code>examples/cnn_train.m</code>). This function requires some options:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-transposed_variable">trainOpts.</span>batchSize = <span class="hljs-number">100</span> ;
<span class="hljs-transposed_variable">trainOpts.</span>numEpochs = <span class="hljs-number">100</span> ;
<span class="hljs-transposed_variable">trainOpts.</span><span class="hljs-keyword">continue</span> = true ;
<span class="hljs-transposed_variable">trainOpts.</span>useGpu = false ;
<span class="hljs-transposed_variable">trainOpts.</span>learningRate = <span class="hljs-number">0.001</span> ;
<span class="hljs-transposed_variable">trainOpts.</span>numEpochs = <span class="hljs-number">15</span> ;
<span class="hljs-transposed_variable">trainOpts.</span>expDir = <span class="hljs-string">'data/chars-experiment'</span> ;</code></pre>

<p>This says that the function will operate on SGD mini-batches of 100 elements,  it will run for 15 epochs (passes through the data),  it will continue from the last epoch if interrupted, if will <em>not</em> use the GPU, it will use a learning rate of 0.001, and it will save any file in the <code>data/chars-experiment</code> subdirectory.</p>

<p>Before the training starts, the average image value is subtracted:</p>



<pre class="prettyprint"><code class=" hljs avrasm">% Take the average image <span class="hljs-keyword">out</span>
imageMean = mean(imdb<span class="hljs-preprocessor">.images</span><span class="hljs-preprocessor">.data</span>(:)) <span class="hljs-comment">;</span>
imdb<span class="hljs-preprocessor">.images</span><span class="hljs-preprocessor">.data</span> = imdb<span class="hljs-preprocessor">.images</span><span class="hljs-preprocessor">.data</span> - imageMean <span class="hljs-comment">;</span></code></pre>

<p>This is similar to what we have done in Part 3.</p>

<p>The training code is called as follows:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Call training function in MatConvNet</span>
<span class="hljs-matrix">[net,info]</span> = cnn_train(net, imdb, @getBatch, trainOpts) ;</code></pre>

<p>Here the key, in addition to the <code>trainOpts</code> structure, is the <code>@getBatch</code> function handle. This is how <code>cnn_train</code> obtains a copy of the data to operate on. Examine this function (see the bottom of the <code>exercise4.m</code> file):</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">[im, labels]</span> = <span class="hljs-title">getBatch</span><span class="hljs-params">(imdb, batch)</span></span>
im = <span class="hljs-transposed_variable">imdb.</span><span class="hljs-transposed_variable">images.</span>data(:,:,batch) ;
im = <span class="hljs-number">256</span> * <span class="hljs-built_in">reshape</span>(im, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>, <span class="hljs-matrix">[]</span>) ;
labels = <span class="hljs-transposed_variable">imdb.</span><span class="hljs-transposed_variable">images.</span>label(<span class="hljs-number">1</span>,batch) ;</code></pre>

<p>The function extracts the <span class="MathJax_Preview">m</span><span class="MathJax MathJax_Processing" id="MathJax-Element-171-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-171">m</script> images corresponding to the vector of indexes <code>batch</code>. It also reshape them as a <span class="MathJax_Preview">32\times 32\times 1\times m</span><span class="MathJax MathJax_Processing" id="MathJax-Element-172-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-172">32\times 32\times 1\times m</script> array (as this is the format expected by the MatConvNet functions) and multiplies the values by 256 (the resulting values match the network initialization and learning parameters). Finally, it also returns a vector of labels, one for each image in the batch.</p>

<blockquote>
  <p><strong>Task:</strong> Run the learning code and exahmine the plots that are produced. As training completes answer the following questions:</p>
  
  <ol>
  <li>How many images per second can you process? (Look at the output in the MATLAB screen)</li>
  <li>There are two sets of curves: energy and prediction error. What do you think is the difference? What is the “energy”?</li>
  <li>Some curves are labelled “train” and some other “val”.  Should they be equal? Which one should be lower than the other?</li>
  <li>Both the top-1 and top-5 prediction errors are plotted. What do they mean? What is the difference?</li>
  </ol>
</blockquote>

<p>Once training is finished, the model is saved back:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Save the result for later use</span>
<span class="hljs-transposed_variable">net.</span>layers(<span class="hljs-keyword">end</span>) = <span class="hljs-matrix">[]</span> ;
<span class="hljs-transposed_variable">net.</span>imageMean = imageMean ;
save(<span class="hljs-string">'data/chars-experiment/charscnn.mat'</span>, <span class="hljs-string">'-struct'</span>, <span class="hljs-string">'net'</span>) ;</code></pre>

<p>Note that we remember the <code>imageMean</code> for later use. Note also that the softmaxloss layer is <em>removed</em> from the network before saving.</p>



<h3 id="part-44-visualize-the-learned-filters">Part 4.4: visualize the learned filters</h3>

<p>The next step is to glance at the filters that have been learned:</p>



<pre class="prettyprint"><code class="language-matlab hljs ">figure(<span class="hljs-number">2</span>) ; clf ; colormap gray ;
vl_imarraysc(<span class="hljs-built_in">squeeze</span>(<span class="hljs-transposed_variable">net.</span>layers<span class="hljs-cell">{<span class="hljs-number">1</span>}.</span>filters),<span class="hljs-string">'spacing'</span>,<span class="hljs-number">2</span>)
axis equal ;
title(<span class="hljs-string">'filters in the first layer'</span>) ;</code></pre>

<blockquote>
  <p><strong>Task:</strong> what can you say about the filters?</p>
</blockquote>



<h3 id="part-45-apply-the-model">Part 4.5: apply the model</h3>

<p>We now apply the model to a whole sequence of characters. This is the image <code>data/sentence-lato.png</code>:</p>

<p><img width="576px" src="images/sentence-lato.png" alt="sentence-lato"></p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Load the CNN learned before</span>
net = load(<span class="hljs-string">'data/chars-experiment/charscnn.mat'</span>) ;

<span class="hljs-comment">% Load the sentence</span>
im = im2single(imread(<span class="hljs-string">'data/sentence-lato.png'</span>)) ;
im = <span class="hljs-number">256</span> * (im - <span class="hljs-transposed_variable">net.</span>imageMean) ;

<span class="hljs-comment">% Apply the CNN to the larger image</span>
res = vl_simplenn(net, im) ;</code></pre>

<blockquote>
  <p><strong>Question:</strong> The image is much wider than 32 pixels. Why can you apply to it the CNN learned before for <span class="MathJax_Preview">32\times 32</span><span class="MathJax MathJax_Processing" id="MathJax-Element-173-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-173">32\times 32</script> patches?</p>
  
  <p><strong>Task:</strong> examine the size of the CNN output using <code>size(res(end).x)</code>. Does this match your expectation?</p>
</blockquote>

<p>Now use the <code>decodeCharacters()</code>  function to visualise the results:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% Visualize the results</span>
figure(<span class="hljs-number">3</span>) ; clf ;
decodeCharacters(net, imdb, im, res) ;</code></pre>

<blockquote>
  <p><strong>Tasks:</strong> inspect the output of the <code>decodeCharacters()</code> function and answer the following:</p>
  
  <ol>
  <li>Is the quality of the recognition any good?</li>
  <li>Does this match your expectation given the recognition rate in your validation set (as reported by <code>cnn_train</code> during training)?</li>
  </ol>
</blockquote>



<h3 id="part-46-training-with-jitter">Part 4.6: training with jitter</h3>

<p>A key issue with the previous CNN is that it is not trained to recognise characters in the context of other characters. Furthermore, characters are perfectly centred in the patch. We can relax these assumptions by making the training data “more realistic”. In this part we will train a second network applying <em>data jittering</em> by:</p>

<ol>
<li>Randomly adding a character to the left and to the right of the one recognised and</li>
<li>Randomly shifting the characters by up to <span class="MathJax_Preview">\pm 5</span><span class="MathJax MathJax_Processing" id="MathJax-Element-174-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-174">\pm 5</script> pixels horizontally and <span class="MathJax_Preview">\pm 2</span><span class="MathJax MathJax_Processing" id="MathJax-Element-175-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-175">\pm 2</script> pixels vertically.</li>
</ol>

<p>This is implemented by the <code>getBatchWithJitter()</code>  function (note that jittering is applied on the fly as it is so fast).</p>

<blockquote>
  <p><strong>Tasks:</strong></p>
  
  <ol>
  <li>Train a second model, using the jittered data.</li>
  <li>Look at the training and validation errors. Is their gap as wide as it was before?</li>
  <li>Use the new model to recognise the characters in the sentence by repeating the previous part. Does it work better?</li>
  <li><strong>Advanced.</strong> What else can you change to make the performance even better?</li>
  </ol>
</blockquote>



<h2 id="part-5-using-pertained-models">Part 5: using pertained models</h2>

<p>A characteristic of deep learning is that it constructs <em>representations</em> of the data. These representations tend to have a universal value, or at least to be applicable to an array of problems that trascends the particular task a model was trained for. This is fortunate as training complex models requires weeks of works on one or more GPUs or hundreds of CPUs; these models can then be frozen and reused for a number of additional applications, with no or minimal additional work.</p>

<p>In this part we will see how MatConvNet can be used to download and run high-performance CNN models for image classification. These models are trained from 1.2M images in the ImageNet datasets to discriminate 1,000 different object categories.</p>

<p>Several <a href="http://www.vlfeat.org/matconvnet/pretrained/">pertained models</a> can be downloaded from the MatConvNet website, including several trained using other CNN implementations such as Caffe. One such models is included in the practical <code>data/imagenet-vgg-verydeep-16.mat</code> file. This is one of the best models from the ImageNet ILSVCR Challenge 2014.</p>



<h3 id="part-51-load-a-pretrained-model">Part 5.1:  load a pretrained model</h3>

<p>The first step is to load the model itself. This is in the format of the <code>vl_simplenn</code> CNN wrapper, and ships as a MATLAB <code>.mat</code> file:</p>



<pre class="prettyprint"><code class="language-matlab hljs ">net = load(<span class="hljs-string">'data/imagenet-vgg-verydeep-16.mat'</span>) ;
vl_simplenn_display(net) ;</code></pre>

<blockquote>
  <p><strong>Tasks:</strong></p>
  
  <ol>
  <li>Look at the output of <code>vl_simplenn_display</code> and understand the structure of the model. Can you understand why it is called “very deep”?</li>
  <li>Look at the size of the file <code>data/imagenet-vgg-verydeep-16.mat</code> on disk. This is <em>just the model</em>.</li>
  </ol>
</blockquote>



<h3 id="part-52-use-the-model-to-classify-an-image">Part 5.2: use the model to classify an image</h3>

<p>We can now use the model to classify an image. We start from <code>peppers.png</code>, a MATLAB stock image:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% obtain and preprocess an image</span>
im = imread(<span class="hljs-string">'peppers.png'</span>) ;
im_ = single(im) ; <span class="hljs-comment">% note: 255 range</span>
im_ = imresize(im_, <span class="hljs-transposed_variable">net.</span><span class="hljs-transposed_variable">normalization.</span>imageSize(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>)) ;
im_ = im_ - <span class="hljs-transposed_variable">net.</span><span class="hljs-transposed_variable">normalization.</span>averageImage ;</code></pre>

<p>The code normalises the image in a format compatible with the model <code>net</code>. This amounts to: converting the image to <code>single</code> format (but with range 0,…,255 rather than [0, 1] as typical in MATLAB), resizing the image to a fixed size, and then subtracting an average image.</p>

<p>It is now possible to call the CNN:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% run the CNN</span>
res = vl_simplenn(net, im_) ;</code></pre>

<p>As usual, <code>res</code> contains the results of the computation, including all intermediate layers. The last one can be used to perform the classification:</p>



<pre class="prettyprint"><code class="language-matlab hljs "><span class="hljs-comment">% show the classification result</span>
scores = <span class="hljs-built_in">squeeze</span>(gather(res(<span class="hljs-keyword">end</span>).x)) ;
<span class="hljs-matrix">[bestScore, best]</span> = max(scores) ;

figure(<span class="hljs-number">1</span>) ; clf ; imagesc(im) ;
title(sprintf(<span class="hljs-string">'%s (%d), score %.3f'</span>,...
  <span class="hljs-transposed_variable">net.</span><span class="hljs-transposed_variable">classes.</span>description<span class="hljs-cell">{best}</span>, best, bestScore)) ;</code></pre>

<p>That completes this practical.</p>



<h2 id="links-and-further-work">Links and further work</h2>

<ul>
<li>The code for this practical is written using the software package <a href="http://www.vlfeat.org/matconvnet">MatConvNet</a>. This is a software library written in MATLAB, C++, and CUDA and is freely available as source code and binary.</li>
<li>The ImageNet model is the <em>VGG very deep 16</em> of Karen Simonyan and Andrew Zisserman.</li>
</ul>



<h2 id="acknowledgements">Acknowledgements</h2>

<ul>
<li>Beta testing by Karel Lenc.</li>
</ul>



<h2 id="history">History</h2>

<ul>
<li>Used in the Oxford AIMS CDT, 2014-15.</li>
</ul>

<div class="footnotes"><hr><ol><li id="fn:lattice">A two-dimensional <em>lattice</em> is a discrete grid embedded in <span class="MathJax_Preview">R^2</span><span class="MathJax MathJax_Processing" id="MathJax-Element-177-Frame" role="textbox" aria-readonly="true"></span><script type="math/tex" id="MathJax-Element-177">R^2</script>, similar for example to a checkerboard. <a href="#fnref:lattice" title="Return to article" class="reversefootnote">↩</a></li></ol></div></div></body>
</html>